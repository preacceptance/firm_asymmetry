{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# sentiment analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## open all tweets in folder of raw tweets\n",
    "\n",
    "tweets = os.listdir(\"raw_tweets_entitites_all_tweets_1000\")\n",
    "tweets = [i for i in tweets if i.endswith(\"json\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### merge all tweets into one big list\n",
    "\n",
    "big = []\n",
    "for i in tweets:\n",
    "    with open(\"all_tweets_1000/\"+i, \"r\") as f:\n",
    "        t = json.load(f)\n",
    "    big.extend(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1013650"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "990338"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### remove empty tweets\n",
    "\n",
    "t = [i for i in big if str(i[\"entity_name\"])!=\"nan\"]\n",
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zhuang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### import other modules\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pdb import set_trace\n",
    "import pandas as pd\n",
    "import ast\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "from cleantext import clean\n",
    "\n",
    "# sentiment analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#lemmatizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'univers univers'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### different stemmers and lemmatizers to experiment with\n",
    "\n",
    "def lemmetize_print(words):\n",
    "    a = []\n",
    "    tokens = word_tokenize(words)\n",
    "    for token in tokens:\n",
    "        lemmetized_word = lemmatizer.lemmatize(token)\n",
    "        a.append(lemmetized_word)\n",
    "    return (\" \".join(a))\n",
    "\n",
    "lemmetize_print(\"cats cats\")\n",
    "\n",
    "\n",
    "\n",
    "def stem_print(words):\n",
    "    a = []\n",
    "    tokens = word_tokenize(words)\n",
    "    for token in tokens:\n",
    "        stemmed_word = snow_stemmer.stem(token)\n",
    "        a.append(stemmed_word)\n",
    "        \n",
    "    return (\" \".join(a))\n",
    "\n",
    "lemmetize_print(\"cats cats\")\n",
    "stem_print(\"universal universe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make list of stop words, add 'rt' to list\n",
    "\n",
    "s = stopwords.words('english')\n",
    "s.append(\"rt\")\n",
    "\n",
    "## remove stop words from stopwords dictionary that are negative or positive (we only want to remove neutral words for sentiment analysis purposes)\n",
    "s = [i for i in s if analyzer.polarity_scores(i)['compound']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove stopwords function\n",
    "def remove_stopwords(sen):\n",
    "\n",
    "    word_tokens = word_tokenize(sen)\n",
    "\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in s]\n",
    "\n",
    "    \" \".join(filtered_sentence)\n",
    "    \n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "### make dataframe of all tweets\n",
    "\n",
    "df = pd.DataFrame(t)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## begin processing text\n",
    "\n",
    "## lowercase all\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: str(x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove urls and handles\n",
    "\n",
    "df['text'] = df['text'].map(lambda x: re.sub(\"@[^ ]+\", \"\", x, count=0, flags=0))\n",
    "df['text'] = df['text'].map(lambda x: re.sub(\"http[^ ]*\", \"\", x, count=0, flags=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove stopwords, add lemmatized and stemmed version\n",
    "\n",
    "df['text_nostop'] = df['text'].map(lambda x: remove_stopwords(x))\n",
    "df['text_nostop_lemma'] = df['text_nostop'].map(lambda x: lemmetize_print(x))\n",
    "df['text_nostop_stem'] = df['text_nostop'].map(lambda x: stem_print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### sentiment analysis of lemmatized and stemmed cleaned tweets\n",
    "\n",
    "\n",
    "df['sentiment_nostop'] = df['text_nostop'].map(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "df['sentiment_nostop_lemma'] = df['text_nostop_lemma'].map(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "df['sentiment_nostop_stem'] = df['text_nostop_stem'].map(lambda x: analyzer.polarity_scores(x)[\"compound\"])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write to json\n",
    "\n",
    "df.to_csv(\"full_dataframe_with_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### write all merged tweets to json in case we need it later\n",
    "\n",
    "with open(\"all_entity_tweets_1000_each.json\", \"w\") as outfile:\n",
    "    json_object = json.dumps(t, indent=4)\n",
    "    outfile.write(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "##  we also tried removing emojis but it made no difference\n",
    "\n",
    "import re\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhuang\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3326: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"full_dataframe_with_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove emojis from text \n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "df['text_nostop_lemma_emoji'] = df['text_nostop_lemma'].map(lambda x: clean(x, no_emoji=True))\n",
    "df['text_nostop_stem_emoji'] = df['text_nostop_stem'].map(lambda x: clean(x, no_emoji=True))\n",
    "\n",
    "df.to_csv(\"full_dataframe_with_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
