## Corporate Essence Analysis - Twitter Acquisition
## , 
# Clear working directory
remove(list = ls())
# Set working directory to current file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Import packages
if (!require(pacman)) {install.packages("pacman")}
pacman::p_load('openxlsx', #open Excel spreadsheets
'ggplot2', #plot stuff
'ggimage', #add images to ggplot legend
'pbkrtest',
'lme4', #linear regression
'data.table', #get percentage of values in a column
'dplyr', #data manipulation
'jtools',
'psych',
'plotrix', #used for std.error() to get standard error
'cowplot', #reads image files into R; add images as x-axis labels
'grid', #used for rasterGrob()
'png', #read PNG files
'filesstrings', #move files around
'effsize', #effect size package
'effectsize',
'ggpubr'
)
##================================================================================================================
##IMPORT & PRE-PROCESS DATA##
##================================================================================================================
# Read data
d_raw <- read.csv("data_raw.csv")
d_raw <- d_raw[2:nrow(d_raw), ] # Subset the last 320 responses (the first response was a test)
d_raw <- d_raw[!(d_raw$ResponseId %in% c("R_3KT2dJa9JNaGXoA", "R_AM9X1caZ4FVC5DX", "R_rkwlVeAVc5OTGmZ", "R_wZT3PBDk0xTY0NP")), ] # Exclude "accidental" participants
n_ss <- dim(d_raw)[1]; n_ss
## -- Pre-process
# Define new data frame that we'll extract preprocessed data into
d_subset <- array(dim=c(n_ss, 3))
colnames(d_subset) <- c('cond_name', 'deactivate', 'essence')
d_subset <- as.data.frame(d_subset, stringsAsFactors=FALSE)
# Extract the good data from the middle part of the raw data
for(i in 1:n_ss) {
cond_name <- d_raw[i,40][!is.na(d_raw[i,40])] #condition name
d_subset[i,1] <- cond_name #condition name
curr <- d_raw[i,24:27][!is.na(d_raw[i,24:27])] #for a given row, get only the non NA values
d_subset[i,2:3] <- as.numeric(curr[curr!= ""]) #and only the non-empty values
}
# Merge good data with first and last halves of the original data
d <- cbind(d_raw[,18:23], d_subset, d_raw[,28:40])
d$ss <- 1:dim(d)[1]
# Get numeric conditions
d$condition <- ifelse(d$cond_name=='buffett', -1, 1)
## -- Exclusions
# Renumber exclusion checks
for(i in 1:n_ss) {
# Comprehension checks
if(d[i,"cond_name"]=='musk') {
if((d[i,"comp_check_1"]==1) & (d[i,"comp_check_2"]==4)) {
d[i,"comp.errors"] <- 0
} else if((d[i,"comp_check_1"]!=1) & (d[i,"comp_check_2"]==4)) {
d[i,"comp.errors"] <- 1
} else if((d[i,"comp_check_1"]==1) & (d[i,"comp_check_2"]!=4)) {
d[i,"comp.errors"] <- 1
} else {
d[i,"comp.errors"] <- 2
}
} else if(d[i,"cond_name"]=='buffett') {
if((d[i,"comp_check_1"]==2) & (d[i,"comp_check_2"]==1)) {
d[i,"comp.errors"] <- 0
} else if((d[i,"comp_check_1"]!=2) & (d[i,"comp_check_2"]==1)) {
d[i,"comp.errors"] <- 1
} else if((d[i,"comp_check_1"]==2) & (d[i,"comp_check_2"]!=1)) {
d[i,"comp.errors"] <- 1
} else {
d[i,"comp.errors"] <- 2
}
}
# Attention checks
# Perform first round of attention checks
d[i, "chk.errors"] <- ifelse(((d[i, "attn_check_1"] == 2) & (d[i, "attn_check_2"] == 2)), 0, 1)
# Perform second round of attention checks, if they failed the first
if(d[i, "chk.errors"] != 0) {
if(((d[i,"attn_check_3_1"]==15) & (d[i,"attn_check_3_2"] > d[i,"attn_check_3_1"]) &
(d[i,"attn_check_3_3"] > d[i,"attn_check_3_2"]) & (d[i,"attn_check_3_2"]%%10 == 0)) &
(d[i,"attn_check_4"]==1)) {
d[i,"chk.errors"] <- 0
} else if(((d[i,"attn_check_3_1"]==15) & (d[i,"attn_check_3_2"] > d[i,"attn_check_3_1"]) &
(d[i,"attn_check_3_3"] > d[i,"attn_check_3_2"]) & (d[i,"attn_check_3_2"]%%10 == 0)) &
(d[i,"attn_check_4"]!=1)) {
d[i,"chk.errors"] <- 1
} else if(((d[i,"attn_check_3_1"]!=15) | (d[i,"attn_check_3_2"] <= d[i,"attn_check_3_1"]) |
(d[i,"attn_check_3_3"] <= d[i,"attn_check_3_2"]) | (d[i,"attn_check_3_2"]%%10 != 0)) &
(d[i,"attn_check_4"]==1)) {
d[i,"chk.errors"] <- 1
} else {
d[i,"chk.errors"] <- 2
}
}
}
# Perform attention exclusions
d <- subset(d, d$chk.errors==0)
n_before_exclusions <- dim(d)[1]; n_before_exclusions
# Perform comprehension exclusions
final_data <- subset(d, d$comp.errors<=1)
n_after_exclusions <- dim(final_data)[1]; n_after_exclusions
exclusion_percent <- ((n_ss - n_after_exclusions)/n_after_exclusions)*100
## -- Demographics
d_age <- mean(as.numeric(final_data$age), na.rm = TRUE); d_age
setDT(final_data)[, 100 * .N/nrow(final_data), by = gender] #female: 47.63% (coded as 2 in raw data)
##=============================================================================================================
## ANALYSES ##
##=============================================================================================================
# Liberal or conservative leaning?
pol_mean <- mean(final_data$politics); pol_mean
pol_sd <- sd(final_data$politics); pol_sd
pol_median <- median(final_data$politics); pol_median
### --- Identity Anova
mod <- aov(deactivate~condition*politics_1, data=final_data)
summary(mod)
eta_squared(mod)
data$polotics_1
data$politics_1
## Corporate Essence Analysis - Twitter Acquisition
## , 
# Clear working directory
remove(list = ls())
# Set working directory to current file location
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Import packages
if (!require(pacman)) {install.packages("pacman")}
pacman::p_load('openxlsx', #open Excel spreadsheets
'ggplot2', #plot stuff
'ggimage', #add images to ggplot legend
'pbkrtest',
'lme4', #linear regression
'data.table', #get percentage of values in a column
'dplyr', #data manipulation
'jtools',
'psych',
'plotrix', #used for std.error() to get standard error
'cowplot', #reads image files into R; add images as x-axis labels
'grid', #used for rasterGrob()
'png', #read PNG files
'filesstrings', #move files around
'effsize', #effect size package
'effectsize',
'ggpubr'
)
##================================================================================================================
##IMPORT & PRE-PROCESS DATA##
##================================================================================================================
# Read data
d_raw <- read.csv("data_raw.csv")
d_raw <- d_raw[2:nrow(d_raw), ] # Subset the last 320 responses (the first response was a test)
d_raw <- d_raw[!(d_raw$ResponseId %in% c("R_3KT2dJa9JNaGXoA", "R_AM9X1caZ4FVC5DX", "R_rkwlVeAVc5OTGmZ", "R_wZT3PBDk0xTY0NP")), ] # Exclude "accidental" participants
n_ss <- dim(d_raw)[1]; n_ss
## -- Pre-process
# Define new data frame that we'll extract preprocessed data into
d_subset <- array(dim=c(n_ss, 3))
colnames(d_subset) <- c('cond_name', 'deactivate', 'essence')
d_subset <- as.data.frame(d_subset, stringsAsFactors=FALSE)
# Extract the good data from the middle part of the raw data
for(i in 1:n_ss) {
cond_name <- d_raw[i,40][!is.na(d_raw[i,40])] #condition name
d_subset[i,1] <- cond_name #condition name
curr <- d_raw[i,24:27][!is.na(d_raw[i,24:27])] #for a given row, get only the non NA values
d_subset[i,2:3] <- as.numeric(curr[curr!= ""]) #and only the non-empty values
}
# Merge good data with first and last halves of the original data
d <- cbind(d_raw[,18:23], d_subset, d_raw[,28:40])
d$ss <- 1:dim(d)[1]
# Get numeric conditions
d$condition <- ifelse(d$cond_name=='buffett', -1, 1)
## -- Exclusions
# Renumber exclusion checks
for(i in 1:n_ss) {
# Comprehension checks
if(d[i,"cond_name"]=='musk') {
if((d[i,"comp_check_1"]==1) & (d[i,"comp_check_2"]==4)) {
d[i,"comp.errors"] <- 0
} else if((d[i,"comp_check_1"]!=1) & (d[i,"comp_check_2"]==4)) {
d[i,"comp.errors"] <- 1
} else if((d[i,"comp_check_1"]==1) & (d[i,"comp_check_2"]!=4)) {
d[i,"comp.errors"] <- 1
} else {
d[i,"comp.errors"] <- 2
}
} else if(d[i,"cond_name"]=='buffett') {
if((d[i,"comp_check_1"]==2) & (d[i,"comp_check_2"]==1)) {
d[i,"comp.errors"] <- 0
} else if((d[i,"comp_check_1"]!=2) & (d[i,"comp_check_2"]==1)) {
d[i,"comp.errors"] <- 1
} else if((d[i,"comp_check_1"]==2) & (d[i,"comp_check_2"]!=1)) {
d[i,"comp.errors"] <- 1
} else {
d[i,"comp.errors"] <- 2
}
}
# Attention checks
# Perform first round of attention checks
d[i, "chk.errors"] <- ifelse(((d[i, "attn_check_1"] == 2) & (d[i, "attn_check_2"] == 2)), 0, 1)
# Perform second round of attention checks, if they failed the first
if(d[i, "chk.errors"] != 0) {
if(((d[i,"attn_check_3_1"]==15) & (d[i,"attn_check_3_2"] > d[i,"attn_check_3_1"]) &
(d[i,"attn_check_3_3"] > d[i,"attn_check_3_2"]) & (d[i,"attn_check_3_2"]%%10 == 0)) &
(d[i,"attn_check_4"]==1)) {
d[i,"chk.errors"] <- 0
} else if(((d[i,"attn_check_3_1"]==15) & (d[i,"attn_check_3_2"] > d[i,"attn_check_3_1"]) &
(d[i,"attn_check_3_3"] > d[i,"attn_check_3_2"]) & (d[i,"attn_check_3_2"]%%10 == 0)) &
(d[i,"attn_check_4"]!=1)) {
d[i,"chk.errors"] <- 1
} else if(((d[i,"attn_check_3_1"]!=15) | (d[i,"attn_check_3_2"] <= d[i,"attn_check_3_1"]) |
(d[i,"attn_check_3_3"] <= d[i,"attn_check_3_2"]) | (d[i,"attn_check_3_2"]%%10 != 0)) &
(d[i,"attn_check_4"]==1)) {
d[i,"chk.errors"] <- 1
} else {
d[i,"chk.errors"] <- 2
}
}
}
# Perform attention exclusions
d <- subset(d, d$chk.errors==0)
n_before_exclusions <- dim(d)[1]; n_before_exclusions
# Perform comprehension exclusions
final_data <- subset(d, d$comp.errors<=1)
n_after_exclusions <- dim(final_data)[1]; n_after_exclusions
exclusion_percent <- ((n_ss - n_after_exclusions)/n_after_exclusions)*100
## -- Demographics
d_age <- mean(as.numeric(final_data$age), na.rm = TRUE); d_age
setDT(final_data)[, 100 * .N/nrow(final_data), by = gender] #female: 47.63% (coded as 2 in raw data)
##=============================================================================================================
## ANALYSES ##
##=============================================================================================================
# Liberal or conservative leaning?
pol_mean <- mean(final_data$politics); pol_mean
pol_sd <- sd(final_data$politics); pol_sd
pol_median <- median(final_data$politics); pol_median
### --- Identity Anova
mod <- aov(deactivate~condition*politics_1, data=final_data)
summary(mod)
eta_squared(mod)
final_data$politics_1
